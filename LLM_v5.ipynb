{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install -q -U transformers==\"4.38.2\"\n",
    "!pip install -q accelerate\n",
    "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install -q -U sentence_transformers\n",
    "!pip install -q -U scann\n",
    "!pip install -q -U wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 20:41:33.882112: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-22 20:41:34.017589: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-22 20:41:34.483533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 20:41:36.135808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scann\n",
    "import wikipediaapi\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig,\n",
    "                         )\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_device():\n",
    "    \"\"\"Define the device to be used by PyTorch\"\"\"\n",
    "\n",
    "    # Get the PyTorch version\n",
    "    torch_version = torch.__version__\n",
    "\n",
    "    # Print the PyTorch version\n",
    "    print(f\"PyTorch version: {torch_version}\", end=\" -- \")\n",
    "\n",
    "    # Check if MPS (Multi-Process Service) device is available on MacOS\n",
    "    if torch.backends.mps.is_available():\n",
    "        # If MPS is available, print a message indicating its usage\n",
    "        print(\"using MPS device on MacOS\")\n",
    "        # Define the device as MPS\n",
    "        defined_device = torch.device(\"mps\")\n",
    "    else:\n",
    "        # If MPS is not available, determine the device based on GPU availability\n",
    "        defined_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Print a message indicating the selected device\n",
    "        print(f\"using {defined_device}\")\n",
    "\n",
    "    # Return the defined device\n",
    "    return defined_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, embedding_model):\n",
    "    \"\"\"Get embeddings for a given text using the provided embedding model\"\"\"\n",
    "    \n",
    "    # Encode the text to obtain embeddings using the provided embedding model\n",
    "    embedding = embedding_model.encode(text, show_progress_bar=False)\n",
    "    \n",
    "    # Convert the embeddings to a list of floats and return\n",
    "    return embedding.tolist()\n",
    "\n",
    "\n",
    "def map2embeddings(data, embedding_model):\n",
    "    \"\"\"Map a list of texts to their embeddings using the provided embedding model\"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over each text in the input data list\n",
    "    no_texts = len(data)\n",
    "    print(f\"Mapping {no_texts} pieces of information\")\n",
    "    for i in tqdm(range(no_texts)):\n",
    "        # Get embeddings for the current text using the provided embedding model\n",
    "        embeddings.append(get_embedding(data[i], embedding_model))\n",
    "    \n",
    "    # Return the list of embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt, EOS_TOKEN):\n",
    "    \"\"\"Clean text by removing specific tokens and redundant spaces\"\"\"\n",
    "    txt = (txt\n",
    "           .replace(EOS_TOKEN, \"\") # Replace the end-of-sentence token with an empty string\n",
    "           .replace(\"**\", \"\")      # Replace double asterisks with an empty string\n",
    "           .replace(\"<pad>\", \"\")   # Replace \"<pad>\" with an empty string\n",
    "           .replace(\"  \", \" \")     # Replace double spaces with single spaces\n",
    "          ).strip()                # Strip leading and trailing spaces from the text\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indefinite_article(role_name):\n",
    "    \"\"\"Check if a role name has a determinative adjective before it, and if not, add the correct one\"\"\"\n",
    "    \n",
    "    # Check if the first word is a determinative adjective\n",
    "    determinative_adjectives = [\"a\", \"an\", \"the\"]\n",
    "    words = role_name.split()\n",
    "    if words[0].lower() not in determinative_adjectives:\n",
    "        # Use \"a\" or \"an\" based on the first letter of the role name\n",
    "        determinative_adjective = \"an\" if words[0][0].lower() in \"aeiou\" else \"a\"\n",
    "        role_name = f\"{determinative_adjective} {role_name}\"\n",
    "\n",
    "    return role_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_summary_and_answer(question, data, searcher, embedding_model, model,\n",
    "                                max_new_tokens=2048, temperature=0.4, role=\"expert\"):\n",
    "    \"\"\"Generate an answer for a given question using context from a dataset\"\"\"\n",
    "    \n",
    "    # Embed the input question using the provided embedding model\n",
    "    embeded_question = np.array(get_embedding(question, embedding_model)).reshape(1, -1)\n",
    "    \n",
    "    # Find similar contexts in the dataset based on the embedded question\n",
    "    neighbors, distances = searcher.search_batched(embeded_question)\n",
    "    \n",
    "    # Extract context from the dataset based on the indices of similar contexts\n",
    "    context = \" \".join([data[pos] for pos in np.ravel(neighbors)])\n",
    "    \n",
    "    # Get the end-of-sentence token from the tokenizer\n",
    "    try:\n",
    "        EOS_TOKEN = model.tokenizer.eos_token\n",
    "    except:\n",
    "        EOS_TOKEN = \"<eos>\"\n",
    "    \n",
    "    # Add a determinative adjective to the role\n",
    "    role = add_indefinite_article(role)\n",
    "    \n",
    "    # Generate a prompt for summarizing the context\n",
    "    prompt = f\"\"\"\n",
    "             Summarize this context: \"{context}\" in order to answer the question \"{question}\" as {role}\\\n",
    "             SUMMARY:\n",
    "             \"\"\".strip() + EOS_TOKEN\n",
    "    \n",
    "    # Generate a summary based on the prompt\n",
    "    results = model.generate_text(prompt, max_new_tokens, temperature)\n",
    "    # print(\"-----------results----------------: \"+str(results)+\"-------------------END-----------------\")\n",
    "    \n",
    "    # Clean the generated summary\n",
    "    # summary = clean_text(results[0].split(\"SUMMARY:\")[-1], EOS_TOKEN)\n",
    "    summary = results\n",
    "\n",
    "    # print(\"-----------Summary----------------: \"+str(summary)+\"-------------------END-----------------\")\n",
    "    # Generate a prompt for providing an answer\n",
    "    prompt = f\"\"\"\n",
    "             Here is the context: {summary}\n",
    "             Using the relevant information from the context \n",
    "             and integrating it with your knowledge,\n",
    "             provide an answer as {role} to the question: {question}.\n",
    "             If the context doesn't provide\n",
    "             any relevant information answer with \n",
    "             [I couldn't find a good match in my\n",
    "             knowledge base for your question, \n",
    "             hence I answer based on my own knowledge] \\\n",
    "             ANSWER:\n",
    "             \"\"\".strip() + EOS_TOKEN\n",
    "\n",
    "    # Generate an answer based on the prompt\n",
    "    results = model.generate_text(prompt, max_new_tokens, temperature)\n",
    "    \n",
    "    # Clean the generated answer\n",
    "    answer = clean_text(results[0].split(\"ANSWER:\")[-1], EOS_TOKEN)\n",
    "\n",
    "    # Return the cleaned answer\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compile the regular expression pattern for better performance\n",
    "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
    "\n",
    "def remove_braces_and_content(text):\n",
    "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
    "    return BRACES_PATTERN.sub('', text)\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"Clean the input string.\"\"\"\n",
    "    \n",
    "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
    "    cleaned_string = ' '.join(input_string.split())\n",
    "    \n",
    "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
    "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
    "    \n",
    "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
    "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Reading prompt ] ...............**Random Forest** is an ensemble learning algorithm that combines multiple decision trees to improve predictive performance. It is a supervised learning technique that can be used for both classification and regression tasks.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "* **Ensemble learning:** Random forest combines multiple decision trees through a voting mechanism to improve overall accuracy and reduce overfitting.\n",
      "* **Decision trees:** Each tree in the forest makes a decision based on a feature, and the final prediction is made by aggregating the results of all trees.\n",
      "* **Feature randomness:** Features are randomly selected for each tree, ensuring that no single feature dominates the learning process.\n",
      "* **Bagging:** Random forest uses a technique called bagging to create multiple training sets by randomly sampling with replacement from the original dataset. This helps to reduce the impact of overfitting.\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "1. **Training:**\n",
      "   - Split the training data into multiple bootstrap samples with replacement.\n",
      "   - For each bootstrap sample, build a decision tree using the selected features.\n",
      "   - Train each tree with the bootstrap sample.\n",
      "\n",
      "2. **Prediction:**\n",
      "   - For a new data point, predict its class or value by aggregating the predictions from all trees in the forest.\n",
      "   - The final prediction is the majority vote or the result of a majority vote.\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "* High accuracy and robustness\n",
      "* Can handle high-dimensional data\n",
      "* Robust to outliers\n",
      "* Can be used for both classification and regression\n",
      "\n",
      "**Disadvantages:**\n",
      "\n",
      "* Can be computationally expensive to train\n",
      "* May be sensitive to the number of trees in the forest\n",
      "* Can be difficult to interpret\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "* Credit risk assessment\n",
      "* Medical diagnosis\n",
      "* Image recognition\n",
      "* Fraud detection\n",
      "* Customer segmentation\n",
      "\n",
      "**In summary,**\n",
      "\n",
      "Random forest is a powerful ensemble learning algorithm that combines the strengths of multiple decision trees to improve predictive performance. It is suitable for both classification and regression tasks and is robust to overfitting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!echo \"Can you explain random forest?\" | ./gemma_cpp/build/gemma -- --tokenizer /home/hunter/courses/fp/gemcp/4/tokenizer.spm --compressed_weights /home/hunter/courses/fp/gemcp/4/2b-it-sfp.sbs --model 2b-it --verbosity 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class GemmaCPP():\n",
    "    \"\"\"Wrapper for the C++ implementation of Gemma\"\"\"\n",
    "    \n",
    "    def __init__(self, gemma_cpp, tokenizer, compressed_weights, model):\n",
    "        self.gemma_cpp = gemma_cpp\n",
    "        self.tokenizer = tokenizer\n",
    "        self.compressed_weights = compressed_weights\n",
    "        self.model = model\n",
    "        \n",
    "    def eliminate_long_dots(self, input_string):\n",
    "        \"\"\"Eliminate long sequences of dots from the input string\"\"\"\n",
    "        # Define a regular expression pattern to match sequences of 2 or more dots\n",
    "        pattern = r'\\.{2,}'\n",
    "\n",
    "        # Replace all occurrences of the pattern with a space\n",
    "        output_string = re.sub(pattern, ' ', input_string)\n",
    "\n",
    "        return output_string.strip()\n",
    "    \n",
    "    def beautify_string(self, input_string):\n",
    "        \"\"\"Clean the input string by removing non-letter characters at the beginning\n",
    "           and isolated letters at the end after multiple spaces\"\"\"\n",
    "        # Remove non-letter characters at the beginning of the string\n",
    "        output_string = re.sub(r'^[^a-zA-Z]+', '', input_string.strip())\n",
    "\n",
    "        # Remove isolated letters at the end of the output string after multiple spaces\n",
    "        output_string = re.sub(r'\\s{3,}(.+)\\Z', '', output_string.strip())\n",
    "\n",
    "        return output_string\n",
    "        \n",
    "    def generate_text(self, prompt, *args, **kwargs):\n",
    "        \"\"\"Generate text using the cpp tokenizer and model\"\"\"\n",
    "\n",
    "        # Define the shell command\n",
    "        prompt = prompt.replace('\"', '').replace(\"'\", \"\")\n",
    "        shell_command = f'echo \"{prompt}\" | {gemma_cpp} -- --tokenizer {tokenizer} --compressed_weights {compressed_weights} --model {model} --verbosity 0'\n",
    "\n",
    "        # Execute the shell command and redirect stdout to the Python script's stdout\n",
    "        process = subprocess.Popen(shell_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "        output_text = \"\"\n",
    "        reading_block = \"[ Reading prompt ]\"\n",
    "        \n",
    "        # Communicate with the process and capture stdout \n",
    "        for k, char in enumerate( iter(lambda: process.stdout.read(1), b'') ):\n",
    "            single_char = char.decode(sys.stdout.encoding)\n",
    "            output_text += single_char\n",
    "            if len(output_text) % 20 == 0:\n",
    "                count_reading_blocks = output_text.count(reading_block)\n",
    "                if count_reading_blocks > 1:\n",
    "                    break\n",
    "                    \n",
    "        # Remove long sequences of dots and the reading block, beautify the string\n",
    "        output_text = output_text.replace(reading_block, \"\")\n",
    "        output_text = self.eliminate_long_dots(output_text)\n",
    "        output_text = self.beautify_string(output_text)\n",
    "        output_text = prompt + output_text\n",
    "        # print(\"---------------outputtext-----------------\"+str(output_text)+\"------------------END-------------------\")\n",
    "        # Return output text\n",
    "        return [output_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AIAssistant():\n",
    "    \"\"\"An AI assistant that interacts with users by providing answers based on a provided knowledge base\"\"\"\n",
    "    \n",
    "    def __init__(self, gemma_model, embeddings_name=\"thenlper/gte-large\", temperature=0.4, role=\"expert\"):\n",
    "        \"\"\"Initialize the AI assistant.\"\"\"\n",
    "        # Initialize attributes\n",
    "        self.embeddings_name = embeddings_name\n",
    "        self.knowledge_base = []\n",
    "        self.temperature = temperature\n",
    "        self.role = role\n",
    "        \n",
    "        # Initialize Gemma model (it can be transformer-based or any other)\n",
    "        self.gemma_model = gemma_model  \n",
    "        \n",
    "        # Load the embedding model\n",
    "        self.embedding_model = SentenceTransformer(self.embeddings_name)\n",
    "        \n",
    "    def store_knowledge_base(self, knowledge_base):\n",
    "        \"\"\"Store the knowledge base\"\"\"\n",
    "        self.knowledge_base=knowledge_base\n",
    "        \n",
    "    def learn_knowledge_base(self, knowledge_base):\n",
    "        \"\"\"Store and index the knowledge based to be used by the assistant\"\"\"\n",
    "        # Storing the knowledge base\n",
    "        self.store_knowledge_base(knowledge_base)\n",
    "        \n",
    "        # Load and index the knowledge base\n",
    "        print(\"Indexing and mapping the knowledge base:\")\n",
    "        embeddings = map2embeddings(self.knowledge_base, self.embedding_model)\n",
    "        self.embeddings = np.array(embeddings).astype(np.float32)\n",
    "        \n",
    "        # Instantiate the searcher for similarity search\n",
    "        self.index_embeddings()\n",
    "        \n",
    "    def index_embeddings(self):\n",
    "        \"\"\"Index the embeddings using ScaNN \"\"\"\n",
    "        self.searcher = (scann.scann_ops_pybind.builder(db=self.embeddings, num_neighbors=10, distance_measure=\"dot_product\")\n",
    "                 .tree(num_leaves=min(self.embeddings.shape[0] // 2, 1000), \n",
    "                       num_leaves_to_search=100, \n",
    "                       training_sample_size=self.embeddings.shape[0])\n",
    "                 .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
    "                 .reorder(100)\n",
    "                 .build()\n",
    "           )\n",
    "        \n",
    "    def query(self, query):\n",
    "        \"\"\"Query the knowledge base of the AI assistant.\"\"\"\n",
    "        # Generate and print an answer to the query\n",
    "        answer = generate_summary_and_answer(query, \n",
    "                                             self.knowledge_base, \n",
    "                                             self.searcher, \n",
    "                                             self.embedding_model, \n",
    "                                             self.gemma_model,\n",
    "                                             temperature=self.temperature,\n",
    "                                             role=self.role)\n",
    "        print(answer)\n",
    "        \n",
    "    def set_temperature(self, temperature):\n",
    "        \"\"\"Set the temperature (creativity) of the AI assistant.\"\"\"\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def set_role(self, role):\n",
    "        \"\"\"Define the answering style of the AI assistant.\"\"\"\n",
    "        self.role = role\n",
    "        \n",
    "    def save_embeddings(self, filename=\"embeddings.npy\"):\n",
    "        \"\"\"Save the embeddings to disk\"\"\"\n",
    "        np.save(filename, self.embeddings)\n",
    "        \n",
    "    def load_embeddings(self, filename=\"embeddings.npy\"):\n",
    "        \"\"\"Load the embeddings from disk and index them\"\"\"\n",
    "        self.embeddings = np.load(filename)\n",
    "        # Re-instantiate the searcher\n",
    "        self.index_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = \"thenlper/gte-large\"\n",
    "gemma_cpp = \"./gemma_cpp/build/gemma\"\n",
    "tokenizer = \"/home/hunter/courses/fp/gemcp/4/tokenizer.spm\"\n",
    "compressed_weights = \"/home/hunter/courses/fp/gemcp/4/2b-it-sfp.sbs\"\n",
    "model = \"2b-it\"\n",
    "\n",
    "# Create an instance of the class AIAssistant based on Gemma C++\n",
    "gemma_ai_assistant = AIAssistant(\n",
    "    gemma_model=GemmaCPP(gemma_cpp, tokenizer, compressed_weights, model),\n",
    "    embeddings_name=embeddings_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 20:44:39.154420: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 15968\n",
      "2024-04-22 20:44:39.284236: W scann/utils/gmm_utils.cc:921] Could not normalize centroid due to zero norm or empty or zero-weight partition.\n",
      "2024-04-22 20:44:40.336456: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 1.181622412s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the previously prepared knowledge base and embeddings\n",
    "wikipedia_data_science_kb = pd.read_csv(\"wikipedia_data_science_kb.csv\")\n",
    "knowledge_base = wikipedia_data_science_kb.wikipedia_text.tolist()\n",
    "# Uploading the knowledge base and embeddings to the AI assistant\n",
    "gemma_ai_assistant.store_knowledge_base(knowledge_base=knowledge_base)\n",
    "gemma_ai_assistant.load_embeddings(filename=\"embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a summary of the context:\n",
      "\n",
      "Linear regression is a statistical method that relates one or more dependent variables (Y) to one or more independent variables (X). The goal is to find a linear relationship between the variables and to use this relationship to make predictions about the dependent variable.\n"
     ]
    }
   ],
   "source": [
    "gemma_ai_assistant.query(\"In short under 50 words , what is linear regression?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
