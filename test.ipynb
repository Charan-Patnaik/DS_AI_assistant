{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install -q -U transformers==\"4.38.2\"\n",
    "!pip install -q accelerate\n",
    "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install -q -U sentence_transformers\n",
    "!pip install -q -U scann\n",
    "!pip install -q -U wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scann\n",
    "import wikipediaapi\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig,\n",
    "                         )\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_device():\n",
    "    \"\"\"Define the device to be used by PyTorch\"\"\"\n",
    "\n",
    "    # Get the PyTorch version\n",
    "    torch_version = torch.__version__\n",
    "\n",
    "    # Print the PyTorch version\n",
    "    print(f\"PyTorch version: {torch_version}\", end=\" -- \")\n",
    "\n",
    "    # Check if MPS (Multi-Process Service) device is available on MacOS\n",
    "    if torch.backends.mps.is_available():\n",
    "        # If MPS is available, print a message indicating its usage\n",
    "        print(\"using MPS device on MacOS\")\n",
    "        # Define the device as MPS\n",
    "        defined_device = torch.device(\"mps\")\n",
    "    else:\n",
    "        # If MPS is not available, determine the device based on GPU availability\n",
    "        defined_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Print a message indicating the selected device\n",
    "        print(f\"using {defined_device}\")\n",
    "\n",
    "    # Return the defined device\n",
    "    return defined_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, embedding_model):\n",
    "    \"\"\"Get embeddings for a given text using the provided embedding model\"\"\"\n",
    "    \n",
    "    # Encode the text to obtain embeddings using the provided embedding model\n",
    "    embedding = embedding_model.encode(text, show_progress_bar=False)\n",
    "    \n",
    "    # Convert the embeddings to a list of floats and return\n",
    "    return embedding.tolist()\n",
    "\n",
    "def map2embeddings(data, embedding_model):\n",
    "    \"\"\"Map a list of texts to their embeddings using the provided embedding model\"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over each text in the input data list\n",
    "    no_texts = len(data)\n",
    "    print(f\"Mapping {no_texts} pieces of information\")\n",
    "    for i in tqdm(range(no_texts)):\n",
    "        # Get embeddings for the current text using the provided embedding model\n",
    "        embeddings.append(get_embedding(data[i], embedding_model))\n",
    "    \n",
    "    # Return the list of embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt, EOS_TOKEN):\n",
    "    \"\"\"Clean text by removing specific tokens and redundant spaces\"\"\"\n",
    "    txt = (txt\n",
    "           .replace(EOS_TOKEN, \"\") # Replace the end-of-sentence token with an empty string\n",
    "           .replace(\"**\", \"\")      # Replace double asterisks with an empty string\n",
    "           .replace(\"<pad>\", \"\")   # Replace \"<pad>\" with an empty string\n",
    "           .replace(\"  \", \" \")     # Replace double spaces with single spaces\n",
    "          ).strip()                # Strip leading and trailing spaces from the text\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indefinite_article(role_name):\n",
    "    \"\"\"Check if a role name has a determinative adjective before it, and if not, add the correct one\"\"\"\n",
    "    \n",
    "    # Check if the first word is a determinative adjective\n",
    "    determinative_adjectives = [\"a\", \"an\", \"the\"]\n",
    "    words = role_name.split()\n",
    "    if words[0].lower() not in determinative_adjectives:\n",
    "        # Use \"a\" or \"an\" based on the first letter of the role name\n",
    "        determinative_adjective = \"an\" if words[0][0].lower() in \"aeiou\" else \"a\"\n",
    "        role_name = f\"{determinative_adjective} {role_name}\"\n",
    "\n",
    "    return role_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaHF():\n",
    "    \"\"\"Wrapper for the Transformers implementation of Gemma\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, max_seq_length=2048):\n",
    "        self.model_name = model_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        # Initialize the model and tokenizer\n",
    "        print(\"\\nInitializing model:\")\n",
    "        self.device = define_device()\n",
    "        self.model, self.tokenizer = self.initialize_model(self.model_name, self.device, self.max_seq_length)\n",
    "        \n",
    "    def initialize_model(self, model_name, device, max_seq_length):\n",
    "        \"\"\"Initialize a 4-bit quantized causal language model (LLM) and tokenizer with specified settings\"\"\"\n",
    "\n",
    "        # Define the data type for computation\n",
    "        compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "        # Define the configuration for quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "        )\n",
    "\n",
    "        # Load the pre-trained model with quantization configuration\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=device,\n",
    "            quantization_config=bnb_config,\n",
    "        )\n",
    "\n",
    "        # Load the tokenizer with specified device and max_seq_length\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=device,\n",
    "            max_seq_length=max_seq_length\n",
    "        )\n",
    "        \n",
    "        # Return the initialized model and tokenizer\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def generate_text(self, prompt, max_new_tokens=2048, temperature=0.0):\n",
    "        \"\"\"Generate text using the instantiated tokenizer and model with specified settings\"\"\"\n",
    "    \n",
    "        # Encode the prompt and convert to PyTorch tensor\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "\n",
    "        # Determine if sampling should be performed based on temperature\n",
    "        do_sample = True if temperature > 0 else False\n",
    "\n",
    "        # Generate text based on the input prompt\n",
    "        outputs = self.model.generate(**input_ids, \n",
    "                                      max_new_tokens=max_new_tokens, \n",
    "                                      do_sample=do_sample, \n",
    "                                      temperature=temperature\n",
    "                                     )\n",
    "\n",
    "        # Decode the generated output into text\n",
    "        results = [self.tokenizer.decode(output) for output in outputs]\n",
    "\n",
    "        # Return the list of generated text results\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_and_answer(question, data, searcher, embedding_model, model,\n",
    "                                max_new_tokens=2048, temperature=0.4, role=\"expert\"):\n",
    "    \"\"\"Generate an answer for a given question using context from a dataset\"\"\"\n",
    "    \n",
    "    # Embed the input question using the provided embedding model\n",
    "    embeded_question = np.array(get_embedding(question, embedding_model)).reshape(1, -1)\n",
    "    \n",
    "    # Find similar contexts in the dataset based on the embedded question\n",
    "    neighbors, distances = searcher.search_batched(embeded_question)\n",
    "    \n",
    "    # Extract context from the dataset based on the indices of similar contexts\n",
    "    context = \" \".join([data[pos] for pos in np.ravel(neighbors)])\n",
    "    \n",
    "    # Get the end-of-sentence token from the tokenizer\n",
    "    try:\n",
    "        EOS_TOKEN = model.tokenizer.eos_token\n",
    "    except:\n",
    "        EOS_TOKEN = \"<eos>\"\n",
    "    \n",
    "    # Add a determinative adjective to the role\n",
    "    role = add_indefinite_article(role)\n",
    "    \n",
    "    # Generate a prompt for summarizing the context\n",
    "    prompt = f\"\"\"\n",
    "             Summarize this context: \"{context}\" in order to answer the question \"{question}\" as {role}\\\n",
    "             SUMMARY:\n",
    "             \"\"\".strip() + EOS_TOKEN\n",
    "    \n",
    "    # Generate a summary based on the prompt\n",
    "    results = model.generate_text(prompt, max_new_tokens, temperature)\n",
    "    \n",
    "    # Clean the generated summary\n",
    "    summary = clean_text(results[0].split(\"SUMMARY:\")[-1], EOS_TOKEN)\n",
    "\n",
    "    # Generate a prompt for providing an answer\n",
    "    prompt = f\"\"\"\n",
    "             Here is the context: {summary}\n",
    "             Using the relevant information from the context \n",
    "             and integrating it with your knowledge,\n",
    "             provide an answer as {role} to the question: {question}.\n",
    "             If the context doesn't provide\n",
    "             any relevant information answer with \n",
    "             [I couldn't find a good match in my\n",
    "             knowledge base for your question, \n",
    "             hence I answer based on my own knowledge] \\\n",
    "             ANSWER:\n",
    "             \"\"\".strip() + EOS_TOKEN\n",
    "\n",
    "    # Generate an answer based on the prompt\n",
    "    results = model.generate_text(prompt, max_new_tokens, temperature)\n",
    "    \n",
    "    # Clean the generated answer\n",
    "    answer = clean_text(results[0].split(\"ANSWER:\")[-1], EOS_TOKEN)\n",
    "\n",
    "    # Return the cleaned answer\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AIAssistant():\n",
    "    \"\"\"An AI assistant that interacts with users by providing answers based on a provided knowledge base\"\"\"\n",
    "    \n",
    "    def __init__(self, gemma_model, embeddings_name=\"thenlper/gte-large\", temperature=0.4, role=\"expert\"):\n",
    "        \"\"\"Initialize the AI assistant.\"\"\"\n",
    "        # Initialize attributes\n",
    "        self.embeddings_name = embeddings_name\n",
    "        self.knowledge_base = []\n",
    "        self.temperature = temperature\n",
    "        self.role = role\n",
    "        \n",
    "        # Initialize Gemma model (it can be transformer-based or any other)\n",
    "        self.gemma_model = gemma_model\n",
    "        \n",
    "        # Load the embedding model\n",
    "        self.embedding_model = SentenceTransformer(self.embeddings_name)\n",
    "        \n",
    "    def store_knowledge_base(self, knowledge_base):\n",
    "        \"\"\"Store the knowledge base\"\"\"\n",
    "        self.knowledge_base=knowledge_base\n",
    "        \n",
    "    def learn_knowledge_base(self, knowledge_base):\n",
    "        \"\"\"Store and index the knowledge based to be used by the assistant\"\"\"\n",
    "        # Storing the knowledge base\n",
    "        self.store_knowledge_base(knowledge_base)\n",
    "        \n",
    "        # Load and index the knowledge base\n",
    "        print(\"Indexing and mapping the knowledge base:\")\n",
    "        embeddings = map2embeddings(self.knowledge_base, self.embedding_model)\n",
    "        self.embeddings = np.array(embeddings).astype(np.float32)\n",
    "        \n",
    "        # Instantiate the searcher for similarity search\n",
    "        self.index_embeddings()\n",
    "        \n",
    "    def index_embeddings(self):\n",
    "        \"\"\"Index the embeddings using ScaNN \"\"\"\n",
    "        self.searcher = (scann.scann_ops_pybind.builder(db=self.embeddings, num_neighbors=10, distance_measure=\"dot_product\")\n",
    "                 .tree(num_leaves=min(self.embeddings.shape[0] // 2, 1000), \n",
    "                       num_leaves_to_search=100, \n",
    "                       training_sample_size=self.embeddings.shape[0])\n",
    "                 .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
    "                 .reorder(100)\n",
    "                 .build()\n",
    "           )\n",
    "        \n",
    "    def query(self, query):\n",
    "        \"\"\"Query the knowledge base of the AI assistant.\"\"\"\n",
    "        # Generate and print an answer to the query\n",
    "        answer = generate_summary_and_answer(query, \n",
    "                                             self.knowledge_base, \n",
    "                                             self.searcher, \n",
    "                                             self.embedding_model, \n",
    "                                             self.gemma_model,\n",
    "                                             temperature=self.temperature,\n",
    "                                             role=self.role)\n",
    "        print(answer)\n",
    "        \n",
    "    def set_temperature(self, temperature):\n",
    "        \"\"\"Set the temperature (creativity) of the AI assistant.\"\"\"\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def set_role(self, role):\n",
    "        \"\"\"Define the answering style of the AI assistant.\"\"\"\n",
    "        self.role = role\n",
    "        \n",
    "    def save_embeddings(self, filename=\"embeddings.npy\"):\n",
    "        \"\"\"Save the embeddings to disk\"\"\"\n",
    "        np.save(filename, self.embeddings)\n",
    "        \n",
    "    def load_embeddings(self, filename=\"embeddings.npy\"):\n",
    "        \"\"\"Load the embeddings from disk and index them\"\"\"\n",
    "        self.embeddings = np.load(filename)\n",
    "        # Re-instantiate the searcher\n",
    "        self.index_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compile the regular expression pattern for better performance\n",
    "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
    "\n",
    "def remove_braces_and_content(text):\n",
    "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
    "    return BRACES_PATTERN.sub('', text)\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"Clean the input string.\"\"\"\n",
    "    \n",
    "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
    "    cleaned_string = ' '.join(input_string.split())\n",
    "    \n",
    "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
    "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
    "    \n",
    "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
    "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
    "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
    "    \n",
    "    # Get the Wikipedia page corresponding to the provided category name\n",
    "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "    \n",
    "    # Initialize an empty list to store page titles\n",
    "    pages = []\n",
    "    \n",
    "    # Check if the category exists\n",
    "    if category.exists():\n",
    "        # Iterate through each article in the category and append its title to the list\n",
    "        for article in category.categorymembers.values():\n",
    "            pages.append(article.title)\n",
    "    \n",
    "    # Return the list of page titles\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_pages(categories):\n",
    "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
    "    \n",
    "    # Create a Wikipedia object\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('Gemma AI Assistant (gemma@example.com)', 'en')\n",
    "    \n",
    "    # Initialize lists to store explored categories and Wikipedia pages\n",
    "    explored_categories = []\n",
    "    wikipedia_pages = []\n",
    "\n",
    "    # Iterate through each category\n",
    "    print(\"- Processing Wikipedia categories:\")\n",
    "    for category_name in categories:\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Get the Wikipedia page corresponding to the category\n",
    "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "        \n",
    "        # Extract Wikipedia pages from the category and extend the list\n",
    "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
    "        \n",
    "        # Add the explored category to the list\n",
    "        explored_categories.append(category_name)\n",
    "\n",
    "    # Extract subcategories and remove duplicate categories\n",
    "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
    "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
    "    \n",
    "    # Explore subcategories recursively\n",
    "    while categories_to_explore:\n",
    "        category_name = categories_to_explore.pop()\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Extract more references from the subcategory\n",
    "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
    "\n",
    "        # Iterate through the references\n",
    "        for ref in more_refs:\n",
    "            # Check if the reference is a category\n",
    "            if \"Category:\" in ref:\n",
    "                new_category = ref.replace(\"Category:\", \"\")\n",
    "                # Add the new category to the explored categories list\n",
    "                if new_category not in explored_categories:\n",
    "                    explored_categories.append(new_category)\n",
    "            else:\n",
    "                # Add the reference to the Wikipedia pages list\n",
    "                if ref not in wikipedia_pages:\n",
    "                    wikipedia_pages.append(ref)\n",
    "\n",
    "    # Initialize a list to store extracted texts\n",
    "    extracted_texts = []\n",
    "    \n",
    "    # Iterate through each Wikipedia page\n",
    "    print(\"- Processing Wikipedia pages:\")\n",
    "    for page_title in tqdm(wikipedia_pages):\n",
    "        try:\n",
    "            # Make a request to the Wikipedia page\n",
    "            page = wiki_wiki.page(page_title)\n",
    "\n",
    "            # Check if the page summary does not contain certain keywords\n",
    "            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n",
    "                # Append the page title and summary to the extracted texts list\n",
    "                if len(page.summary) > len(page.title):\n",
    "                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
    "\n",
    "                # Iterate through the sections in the page\n",
    "                for section in page.sections:\n",
    "                    # Append the page title and section text to the extracted texts list\n",
    "                    if len(section.text) > len(page.title):\n",
    "                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {page.title}: {e}\")\n",
    "                    \n",
    "    # Return the extracted texts\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Processing Wikipedia categories:\n",
      "\tExploring Machine_learning on Wikipedia\n",
      "\tExploring Data_science on Wikipedia\n",
      "\tExploring Statistics on Wikipedia\n",
      "\tExploring Deep_learning on Wikipedia\n",
      "\tExploring Artificial_intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence stubs on Wikipedia\n",
      "\tExploring Works created using artificial intelligence on Wikipedia\n",
      "\tExploring Virtual assistants on Wikipedia\n",
      "\tExploring Turing tests on Wikipedia\n",
      "\tExploring AI software on Wikipedia\n",
      "\tExploring Rule engines on Wikipedia\n",
      "\tExploring Artificial intelligence publications on Wikipedia\n",
      "\tExploring Philosophy of artificial intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence people on Wikipedia\n",
      "\tExploring Open-source artificial intelligence on Wikipedia\n",
      "\tExploring Neural networks on Wikipedia\n",
      "\tExploring Multi-agent systems on Wikipedia\n",
      "\tExploring Mind–body problem on Wikipedia\n",
      "\tExploring Machine learning on Wikipedia\n",
      "\tExploring Artificial intelligence laboratories on Wikipedia\n",
      "\tExploring Knowledge representation on Wikipedia\n",
      "\tExploring History of artificial intelligence on Wikipedia\n",
      "\tExploring Generative artificial intelligence on Wikipedia\n",
      "\tExploring Game artificial intelligence on Wikipedia\n",
      "\tExploring Fuzzy logic on Wikipedia\n",
      "\tExploring Fiction about artificial intelligence on Wikipedia\n",
      "\tExploring Existential risk from artificial general intelligence on Wikipedia\n",
      "\tExploring Evolutionary computation on Wikipedia\n",
      "\tExploring Artificial intelligence entertainment on Wikipedia\n",
      "\tExploring Distributed artificial intelligence on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computer vision on Wikipedia\n",
      "\tExploring Artificial intelligence competitions on Wikipedia\n",
      "\tExploring AI companies on Wikipedia\n",
      "\tExploring Cognitive architecture on Wikipedia\n",
      "\tExploring Cloud robotics on Wikipedia\n",
      "\tExploring Chatbots on Wikipedia\n",
      "\tExploring Automated reasoning on Wikipedia\n",
      "\tExploring Artificial intelligence associations on Wikipedia\n",
      "\tExploring Artificial intelligence templates on Wikipedia\n",
      "\tExploring Artificial immune systems on Wikipedia\n",
      "\tExploring Artificial intelligence art on Wikipedia\n",
      "\tExploring Argument technology on Wikipedia\n",
      "\tExploring Applications of artificial intelligence on Wikipedia\n",
      "\tExploring Ambient intelligence on Wikipedia\n",
      "\tExploring AI accelerators on Wikipedia\n",
      "\tExploring Affective computing on Wikipedia\n",
      "\tExploring Text-to-image generation on Wikipedia\n",
      "\tExploring Google DeepMind on Wikipedia\n",
      "\tExploring Deepfakes on Wikipedia\n",
      "\tExploring Deep learning software on Wikipedia\n",
      "\tExploring Statistics stubs on Wikipedia\n",
      "\tExploring Statistical concepts on Wikipedia\n",
      "\tExploring Statistical software on Wikipedia\n",
      "\tExploring Statistical methods on Wikipedia\n",
      "\tExploring Statistical data on Wikipedia\n",
      "\tExploring Subfields of statistics on Wikipedia\n",
      "\tExploring Statistics profession and organizations on Wikipedia\n",
      "\tExploring Statistics-related lists on Wikipedia\n",
      "\tExploring Statisticians on Wikipedia\n",
      "\tExploring Data scientists on Wikipedia\n",
      "\tExploring Unsupervised learning on Wikipedia\n",
      "\tExploring Support vector machines on Wikipedia\n",
      "\tExploring Supervised learning on Wikipedia\n",
      "\tExploring Structured prediction on Wikipedia\n",
      "\tExploring Statistical natural language processing on Wikipedia\n",
      "\tExploring Semisupervised learning on Wikipedia\n",
      "\tExploring Natural language processing researchers on Wikipedia\n",
      "\tExploring Machine learning researchers on Wikipedia\n",
      "\tExploring Reinforcement learning on Wikipedia\n",
      "\tExploring Ontology learning (computer science) on Wikipedia\n",
      "\tExploring Markov models on Wikipedia\n",
      "\tExploring Machine learning task on Wikipedia\n",
      "\tExploring Machine learning algorithms on Wikipedia\n",
      "\tExploring Loss functions on Wikipedia\n",
      "\tExploring Log-linear models on Wikipedia\n",
      "\tExploring Learning in computer vision on Wikipedia\n",
      "\tExploring Latent variable models on Wikipedia\n",
      "\tExploring Kernel methods for machine learning on Wikipedia\n",
      "\tExploring Inductive logic programming on Wikipedia\n",
      "\tExploring Genetic programming on Wikipedia\n",
      "\tExploring Evolutionary algorithms on Wikipedia\n",
      "\tExploring Ensemble learning on Wikipedia\n",
      "\tExploring Dimension reduction on Wikipedia\n",
      "\tExploring Datasets in machine learning on Wikipedia\n",
      "\tExploring Data mining and machine learning software on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computational learning theory on Wikipedia\n",
      "\tExploring Cluster analysis on Wikipedia\n",
      "\tExploring Classification algorithms on Wikipedia\n",
      "\tExploring Blockmodeling on Wikipedia\n",
      "\tExploring Bayesian networks on Wikipedia\n",
      "\tExploring Artificial neural networks on Wikipedia\n",
      "\tExploring Applied machine learning on Wikipedia\n",
      "- Processing Wikipedia pages:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3388/3388 [05:40<00:00,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15968 Wikipedia pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\n",
    "extracted_texts = get_wikipedia_pages(categories)\n",
    "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikipedia_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VACUUM : VACUUM is a set of normative guidance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer audition : Computer audition (CA) or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computer audition : Like computer vision versu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computer audition : Computer Audition overlaps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computer audition : Since audio signals are in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      wikipedia_text\n",
       "0  VACUUM : VACUUM is a set of normative guidance...\n",
       "1  Computer audition : Computer audition (CA) or ...\n",
       "2  Computer audition : Like computer vision versu...\n",
       "3  Computer audition : Computer Audition overlaps...\n",
       "4  Computer audition : Since audio signals are in..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_data_science_kb = pd.DataFrame(extracted_texts, columns=[\"wikipedia_text\"])\n",
    "wikipedia_data_science_kb.to_csv(\"wikipedia_data_science_kb.csv\", index=False)\n",
    "wikipedia_data_science_kb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! export KAGGLE_USERNAME=\"heyitsrj\"\n",
    "# ! export KAGGLE_KEY= \"71e34adb547ddfcbb16ed8c6aecac9ae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9406d18732a4bf7a88bbe5c39c1d3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kaggle credentials set.\n",
      "Kaggle credentials successfully validated.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/gemmaCpp/2b-it-sfp/4/download...\n",
      "100%|██████████| 2.16G/2.16G [00:50<00:00, 46.0MB/s]\n",
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: /home/hunter/.cache/kagglehub/models/google/gemma/gemmaCpp/2b-it-sfp/4\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.model_download(\"google/gemma/gemmaCpp/2b-it-sfp\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gemma_cpp.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile gemma_cpp.sh\n",
    "\n",
    "echo \"/usr/local/lib\" | tee -a /etc/ld.so.conf\n",
    "ldconfig -v \n",
    "\n",
    "\n",
    "\n",
    "git clone https://github.com/google/gemma.cpp\n",
    "cd \"gemma.cpp/build\"\n",
    "cmake ..\n",
    "make -j 4 gemma\n",
    "cd ../..\n",
    "cp -r gemma.cpp/build ./gemma_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tee: /etc/ld.so.conf: Permission denied\n",
      "/usr/local/lib\n",
      "/sbin/ldconfig.real: Can't stat /usr/local/lib/x86_64-linux-gnu: No such file or directory\n",
      "/sbin/ldconfig.real: Path `/usr/lib/x86_64-linux-gnu' given more than once\n",
      "(from /etc/ld.so.conf.d/x86_64-linux-gnu.conf:4 and /etc/ld.so.conf.d/x86_64-linux-gnu.conf:3)\n",
      "/sbin/ldconfig.real: Path `/usr/local/lib' given more than once\n",
      "(from /etc/ld.so.conf:3 and /etc/ld.so.conf.d/libc.conf:2)\n",
      "/sbin/ldconfig.real: Path `/lib/x86_64-linux-gnu' given more than once\n",
      "(from <builtin>:0 and /etc/ld.so.conf.d/x86_64-linux-gnu.conf:3)\n",
      "/sbin/ldconfig.real: Path `/usr/lib/x86_64-linux-gnu' given more than once\n",
      "(from <builtin>:0 and /etc/ld.so.conf.d/x86_64-linux-gnu.conf:3)\n",
      "/sbin/ldconfig.real: Path `/usr/lib' given more than once\n",
      "(from <builtin>:0 and <builtin>:0)\n",
      "/usr/lib/x86_64-linux-gnu/libfakeroot: (from /etc/ld.so.conf.d/fakeroot-x86_64-linux-gnu.conf:1)\n",
      "\tlibfakeroot-0.so -> libfakeroot-tcp.so\n",
      "/usr/lib/wsl/lib: (from /etc/ld.so.conf.d/ld.wsl.conf:4)\n",
      "\tlibdxcore.so -> libdxcore.so\n",
      "\tlibd3d12core.so -> libd3d12core.so\n",
      "\tlibd3d12.so -> libd3d12.so\n",
      "/usr/local/lib: (from /etc/ld.so.conf.d/libc.conf:2)\n",
      "/lib/x86_64-linux-gnu: (from /etc/ld.so.conf.d/x86_64-linux-gnu.conf:3)\n",
      "\tlibcrypto.so.3 -> libcrypto.so.3\n",
      "\tlibk5crypto.so.3 -> libk5crypto.so.3.1\n",
      "\tlibxml2.so.2 -> libxml2.so.2.9.13\n",
      "\tlibisccc-9.18.18-0ubuntu0.22.04.2-Ubuntu.so -> libisccc-9.18.18-0ubuntu0.22.04.2-Ubuntu.so\n",
      "\tlibanl.so.1 -> libanl.so.1\n",
      "\tlibrsvg-2.so.2 -> librsvg-2.so.2.48.0\n",
      "\tlibkrb5support.so.0 -> libkrb5support.so.0.1\n",
      "\tlibgpm.so.2 -> libgpm.so.2\n",
      "\tlibapt-private.so.0.0 -> libapt-private.so.0.0.0\n",
      "\tlibss.so.2 -> libss.so.2.0\n",
      "\tlibdruntime-ldc-shared.so.98 -> libdruntime-ldc-shared.so.2.0.98\n",
      "\tlibnss_compat.so.2 -> libnss_compat.so.2\n",
      "\tlibmemusage.so -> libmemusage.so\n",
      "\tlibgcc_s.so.1 -> libgcc_s.so.1\n",
      "\tlibjbig.so.0 -> libjbig.so.0\n",
      "\tlibwrap.so.0 -> libwrap.so.0.7.6\n",
      "\tlibX11-xcb.so.1 -> libX11-xcb.so.1.0.0\n",
      "\tlibseccomp.so.2 -> libseccomp.so.2.5.3\n",
      "\tlibnftnl.so.11 -> libnftnl.so.11.6.0\n",
      "\tlibresolv.so.2 -> libresolv.so.2\n",
      "\tlibpsl.so.5 -> libpsl.so.5.3.2\n",
      "\tlibtsan.so.0 -> libtsan.so.0.0.0\n",
      "\tlibcares.so.2 -> libcares.so.2.5.1\n",
      "\tlibicudata.so.70 -> libicudata.so.70.1\n",
      "\tliblua5.1-lpeg.so.2 -> liblua5.1-lpeg.so.2.0.0\n",
      "\tlibm.so.6 -> libm.so.6\n",
      "\tlibpcreposix.so.3 -> libpcreposix.so.3.13.3\n",
      "\tlibdns-9.18.18-0ubuntu0.22.04.2-Ubuntu.so -> libdns-9.18.18-0ubuntu0.22.04.2-Ubuntu.so\n",
      "\tlibgstcheck-1.0.so.0 -> libgstcheck-1.0.so.0.2003.0\n",
      "\tlibsensors.so.5 -> libsensors.so.5.0.0\n",
      "\tlibc_malloc_debug.so.0 -> libc_malloc_debug.so.0\n",
      "\tlibXcursor.so.1 -> libXcursor.so.1.0.2\n",
      "\tlibidn2.so.0 -> libidn2.so.0.3.7\n",
      "\tlibnss_hesiod.so.2 -> libnss_hesiod.so.2\n",
      "\tlibpolkit-gobject-1.so.0 -> libpolkit-gobject-1.so.0.0.0\n",
      "\tlibpanel.so.6 -> libpanel.so.6.3\n",
      "\tlibnss_dns.so.2 -> libnss_dns.so.2\n",
      "\tlibkrb5.so.3 -> libkrb5.so.3.3\n",
      "\tlibtirpc.so.3 -> libtirpc.so.3.0.0\n",
      "\tlibGLdispatch.so.0 -> libGLdispatch.so.0.0.0\n",
      "\tlibxcb.so.1 -> libxcb.so.1.1.0\n",
      "\tlibxcb-present.so.0 -> libxcb-present.so.0.0.0\n",
      "\tlibdconf.so.1 -> libdconf.so.1.0.0\n",
      "\tlibksba.so.8 -> libksba.so.8.14.0\n",
      "\tlibBrokenLocale.so.1 -> libBrokenLocale.so.1\n",
      "\tlibyaml-0.so.2 -> libyaml-0.so.2.0.6\n",
      "\tlibldc-jit.so.98 -> libldc-jit.so.2.0.98\n",
      "\tlibexpatw.so.1 -> libexpatw.so.1.8.7\n",
      "\tlibLLVM-11.so.1 -> libLLVM-11.so.1\n",
      "\tlibply-boot-client.so.5 -> libply-boot-client.so.5.0.0\n",
      "\tlibmount.so.1 -> libmount.so.1.1.0\n",
      "\tlibfreetype.so.6 -> libfreetype.so.6.18.1\n",
      "\tlibelf.so.1 -> libelf-0.186.so\n",
      "\tlibrhash.so.0 -> librhash.so.0\n",
      "\tlibICE.so.6 -> libICE.so.6.3.0\n",
      "\tlibbrotlicommon.so.1 -> libbrotlicommon.so.1.0.9\n",
      "\tlibnetplan.so.0.0 -> libnetplan.so.0.0\n",
      "\tlibwayland-egl.so.1 -> libwayland-egl.so.1.20.0\n",
      "\tlibicui18n.so.70 -> libicui18n.so.70.1\n",
      "\tlibstemmer.so.0d -> libstemmer.so.0d.0.0\n",
      "\tlibnpth.so.0 -> libnpth.so.0.1.2\n",
      "\tlibpython3.10.so.1.0 -> libpython3.10.so.1.0\n",
      "\tlibquadmath.so.0 -> libquadmath.so.0.0.0\n",
      "\tlibsodium.so.23 -> libsodium.so.23.3.0\n",
      "\tlibhistory.so.8 -> libhistory.so.8.1\n",
      "\tlibdb-5.3.so -> libdb-5.3.so\n",
      "\tlibcrypt.so.1 -> libcrypt.so.1.1.0\n",
      "\tlibubsan.so.1 -> libubsan.so.1.0.0\n",
      "\tlibpcap.so.0.8 -> libpcap.so.1.10.1\n",
      "\tlibatomic.so.1 -> libatomic.so.1.2.0\n",
      "\tlibmagic.so.1 -> libmagic.so.1.0.0\n",
      "\tlibexpat.so.1 -> libexpat.so.1.8.7\n",
      "\tlibmvec.so.1 -> libmvec.so.1\n",
      "\tlibcups.so.2 -> libcups.so.2\n",
      "\tlibargon2.so.1 -> libargon2.so.1\n",
      "\tlibgdk_pixbuf-2.0.so.0 -> libgdk_pixbuf-2.0.so.0.4200.8\n",
      "\tlibicuio.so.70 -> libicuio.so.70.1\n",
      "\tlibfuse3.so.3 -> libfuse3.so.3.10.5\n",
      "\tlibnss_systemd.so.2 -> libnss_systemd.so.2\n",
      "\tlibmpdec++.so.3 -> libmpdec++.so.2.5.1\n",
      "/sbin/ldconfig.real: /lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 is the dynamic linker, ignoring\n",
      "\n",
      "\tld-linux-x86-64.so.2 -> ld-linux-x86-64.so.2\n",
      "\tlibavahi-client.so.3 -> libavahi-client.so.3.2.9\n",
      "\tlibXcomposite.so.1 -> libXcomposite.so.1.0.0\n",
      "\tlibgmp.so.10 -> libgmp.so.10.4.1\n",
      "\tlibxkbcommon.so.0 -> libxkbcommon.so.0.0.0\n",
      "\tlibpackagekit-glib2.so.18 -> libpackagekit-glib2.so.18.1.3\n",
      "\tlibestr.so.0 -> libestr.so.0.0.0\n",
      "\tlibXrandr.so.2 -> libXrandr.so.2.2.0\n",
      "\tlibxcb-dri3.so.0 -> libxcb-dri3.so.0.0.0\n",
      "\tlibcolordprivate.so.2 -> libcolordprivate.so.2.0.5\n",
      "\tlibctf.so.0 -> libctf.so.0.0.0\n",
      "\tlibxmlb.so.2 -> libxmlb.so.2.0.0\n",
      "\tlibisccfg-9.18.18-0ubuntu0.22.04.2-Ubuntu.so -> libisccfg-9.18.18-0ubuntu0.22.04.2-Ubuntu.so\n",
      "\tlibXinerama.so.1 -> libXinerama.so.1.0.0\n",
      "\tlibjansson.so.4 -> libjansson.so.4.13.0\n",
      "\tlibxshmfence.so.1 -> libxshmfence.so.1.0.0\n",
      "\tlibattr.so.1 -> libattr.so.1.1.2501\n",
      "\tlibbrotlienc.so.1 -> libbrotlienc.so.1.0.9\n",
      "\tlibharfbuzz.so.0 -> libharfbuzz.so.0.20704.0\n",
      "\tlibXext.so.6 -> libXext.so.6.4.0\n",
      "\tlibedit.so.2 -> libedit.so.2.0.68\n",
      "\tlibsqlite3.so.0 -> libsqlite3.so.0.8.6\n",
      "\tlibpng16.so.16 -> libpng16.so.16.37.0\n",
      "\tlibbind9-9.18.18-0ubuntu0.22.04.2-Ubuntu.so -> libbind9-9.18.18-0ubuntu0.22.04.2-Ubuntu.so\n",
      "\tliblcms2.so.2 -> liblcms2.so.2.0.12\n",
      "\tlibdrm_amdgpu.so.1 -> libdrm_amdgpu.so.1.0.0\n",
      "\tlibwebp.so.7 -> libwebp.so.7.1.3\n",
      "\tlibbz2.so.1.0 -> libbz2.so.1.0.4\n",
      "\tlibthread_db.so.1 -> libthread_db.so.1\n",
      "\tlibuuid.so.1 -> libuuid.so.1.3.0\n",
      "\tlibdl.so.2 -> libdl.so.2\n",
      "\tlibavahi-common.so.3 -> libavahi-common.so.3.5.4\n",
      "\tliblzma.so.5 -> liblzma.so.5.2.5\n",
      "\tlibSM.so.6 -> libSM.so.6.0.1\n",
      "\tlibcbor.so.0.8 -> libcbor.so.0.8.0\n",
      "\tlibthai.so.0 -> libthai.so.0.3.1\n",
      "\tlibxcb-dri2.so.0 -> libxcb-dri2.so.0.0.0\n",
      "\tliblinear.so.4 -> liblinear.so.4.2.\n",
      "\tlibgstreamer-1.0.so.0 -> libgstreamer-1.0.so.0.2003.0\n",
      "\tlibdrm_nouveau.so.2 -> libdrm_nouveau.so.2.0.0\n",
      "\tlibtic.so.6 -> libtic.so.6.3\n",
      "\tlibpopt.so.0 -> libpopt.so.0.0.1\n",
      "\tlibXxf86dga.so.1 -> libXxf86dga.so.1.0.0\n",
      "\tlibXxf86vm.so.1 -> libXxf86vm.so.1.0.0\n",
      "\tlibjsoncpp.so.25 -> libjsoncpp.so.1.9.5\n",
      "\tlibsystemd.so.0 -> libsystemd.so.0.32.0\n",
      "\tlibicuuc.so.70 -> libicuuc.so.70.1\n",
      "\tlibnftables.so.1 -> libnftables.so.1.1.0\n",
      "\tlibdeflate.so.0 -> libdeflate.so.0\n",
      "\tlibip4tc.so.2 -> libip4tc.so.2.0.0\n",
      "\tlibXdamage.so.1 -> libXdamage.so.1.1.0\n",
      "\tlibXtst.so.6 -> libXtst.so.6.1.0\n",
      "\tliblz4.so.1 -> liblz4.so.1.9.3\n",
      "\tlibmpdec.so.3 -> libmpdec.so.2.5.1\n",
      "\tlibxkbfile.so.1 -> libxkbfile.so.1.0.2\n",
      "\tlibext2fs.so.2 -> libext2fs.so.2.4\n",
      "\tlibp11-kit.so.0 -> libp11-kit.so.0.3.0\n",
      "\tlibply-splash-core.so.5 -> libply-splash-core.so.5.0.0\n",
      "\tlibpamc.so.0 -> libpamc.so.0.82.1\n",
      "\tlibslang.so.2 -> libslang.so.2.3.2\n",
      "\tlibasan.so.6 -> libasan.so.6.0.0\n",
      "\tlibXau.so.6 -> libXau.so.6.0.0\n",
      "\tlibfribidi.so.0 -> libfribidi.so.0.4.0\n",
      "\tlibunwind.so.8 -> libunwind.so.8.0.1\n",
      "\tlibnuma.so.1 -> libnuma.so.1.0.0\n",
      "\tlibXi.so.6 -> libXi.so.6.1.0\n",
      "\tlibXmu.so.6 -> libXmu.so.6.2.0\n",
      "\tlibisl.so.23 -> libisl.so.23.1.0\n",
      "\tliblua5.3-lpeg.so.2 -> liblua5.3-lpeg.so.2.0.0\n",
      "\tlibicutu.so.70 -> libicutu.so.70.1\n",
      "\tlibrtmp.so.1 -> librtmp.so.1\n",
      "\tlibform.so.6 -> libform.so.6.3\n",
      "\tlibwayland-cursor.so.0 -> libwayland-cursor.so.0.20.0\n",
      "\tlibffi.so.8 -> libffi.so.8.1.0\n",
      "\tlibmpfr.so.6 -> libmpfr.so.6.1.0\n",
      "\tlibisc-9.18.18-0ubuntu0.22.04.2-Ubuntu.so -> libisc-9.18.18-0ubuntu0.22.04.2-Ubuntu.so\n",
      "\tlibnss_files.so.2 -> libnss_files.so.2\n",
      "\tlibnghttp2.so.14 -> libnghttp2.so.14.20.1\n",
      "\tlibe2p.so.2 -> libe2p.so.2.3\n",
      "\tlibgcrypt.so.20 -> libgcrypt.so.20.3.4\n",
      "\tlibpam.so.0 -> libpam.so.0.85.1\n",
      "\tlibsigsegv.so.2 -> libsigsegv.so.2.0.6\n",
      "\tlibcurl.so.4 -> libcurl.so.4.7.0\n",
      "\tlibgmodule-2.0.so.0 -> libgmodule-2.0.so.0.7200.4\n",
      "\tlibtasn1.so.6 -> libtasn1.so.6.6.2\n",
      "\tlibgtk-3.so.0 -> libgtk-3.so.0.2404.29\n",
      "\tlibcap.so.2 -> libcap.so.2.44\n",
      "\tlibmpc.so.3 -> libmpc.so.3.2.1\n",
      "\tlibpciaccess.so.0 -> libpciaccess.so.0.11.1\n",
      "\tlibdrm_intel.so.1 -> libdrm_intel.so.1.0.0\n",
      "\tlibpci.so.3 -> libpci.so.3.7.0\n",
      "\tlibjpeg.so.8 -> libjpeg.so.8.2.2\n",
      "\tlibblkid.so.1 -> libblkid.so.1.1.0\n",
      "\tlibpam_misc.so.0 -> libpam_misc.so.0.82.1\n",
      "\tlibxcb-xfixes.so.0 -> libxcb-xfixes.so.0.0.0\n",
      "\tlibpthread.so.0 -> libpthread.so.0\n",
      "\tlibkeyutils.so.1 -> libkeyutils.so.1.9\n",
      "\tlibatspi.so.0 -> libatspi.so.0.0.1\n",
      "\tlibc.so.6 -> libc.so.6\n",
      "\tlibuchardet.so.0 -> libuchardet.so.0.0.7\n",
      "\tliblmdb.so.0 -> liblmdb.so.0.0.0\n",
      "\tlibselinux.so.1 -> libselinux.so.1\n",
      "\tlibwayland-client.so.0 -> libwayland-client.so.0.20.0\n",
      "\tlibutempter.so.0 -> libutempter.so.1.2.1\n",
      "\tlibXmuu.so.1 -> libXmuu.so.1.0.0\n",
      "\tlibbrotlidec.so.1 -> libbrotlidec.so.1.0.9\n",
      "\tlibxcb-glx.so.0 -> libxcb-glx.so.0.0.0\n",
      "\tlibXv.so.1 -> libXv.so.1.0.0\n",
      "\tlibpangocairo-1.0.so.0 -> libpangocairo-1.0.so.0.5000.6\n",
      "\tlibrt.so.1 -> librt.so.1\n",
      "\tlibsmartcols.so.1 -> libsmartcols.so.1.1.0\n",
      "\tlibpolkit-agent-1.so.0 -> libpolkit-agent-1.so.0.0.0\n",
      "\tlibXfixes.so.3 -> libXfixes.so.3.1.0\n",
      "\tlibformw.so.6 -> libformw.so.6.3\n",
      "\tlibacl.so.1 -> libacl.so.1.1.2301\n",
      "\tlibunwind-coredump.so.0 -> libunwind-coredump.so.0.0.0\n",
      "\tlibkmod.so.2 -> libkmod.so.2.3.7\n",
      "\tlibssl.so.3 -> libssl.so.3\n",
      "\tlibip6tc.so.2 -> libip6tc.so.2.0.0\n",
      "\tlibnetfilter_conntrack.so.3 -> libnetfilter_conntrack.so.3.8.0\n",
      "\tlibhogweed.so.6 -> libhogweed.so.6.4\n",
      "\tlibpangoft2-1.0.so.0 -> libpangoft2-1.0.so.0.5000.6\n",
      "\tlibutil.so.1 -> libutil.so.1\n",
      "\tlibXaw.so.7 -> libXaw7.so.7.0.0\n",
      "\tlibirs-9.18.18-0ubuntu0.22.04.2-Ubuntu.so -> libirs-9.18.18-0ubuntu0.22.04.2-Ubuntu.so\n",
      "\tlibsasl2.so.2 -> libsasl2.so.2.0.25\n",
      "\tlibmaxminddb.so.0 -> libmaxminddb.so.0.0.7\n",
      "\tlibLLVM-15.so.1 -> libLLVM-15.so.1\n",
      "\tlibatk-1.0.so.0 -> libatk-1.0.so.0.23609.1\n",
      "\tliblber-2.5.so.0 -> liblber-2.5.so.0.1.11\n",
      "\tlibnl-genl-3.so.200 -> libnl-genl-3.so.200.26.0\n",
      "\tlibgio-2.0.so.0 -> libgio-2.0.so.0.7200.4\n",
      "\tlibGL.so.1 -> libGL.so.1.7.0\n",
      "\tlibdns-export.so.1110 -> libdns-export.so.1110.0.2\n",
      "\tlibapparmor.so.1 -> libapparmor.so.1.8.2\n",
      "\tlibdrm_radeon.so.1 -> libdrm_radeon.so.1.0.1\n",
      "\tlibapt-pkg.so.6.0 -> libapt-pkg.so.6.0.0\n",
      "\tlibxcb-shm.so.0 -> libxcb-shm.so.0.0.0\n",
      "\tlibcairo-gobject.so.2 -> libcairo-gobject.so.2.11600.0\n",
      "\tlibpcre.so.3 -> libpcre.so.3.13.3\n",
      "\tlibitm.so.1 -> libitm.so.1.0.0\n",
      "\tlibpcprofile.so -> libpcprofile.so\n",
      "\tlibldap-2.5.so.0 -> libldap-2.5.so.0.1.11\n",
      "\tlibgdbm_compat.so.4 -> libgdbm_compat.so.4.0.0\n",
      "\tlibnsl.so.1 -> libnsl.so.1\n",
      "\tlibply-splash-graphics.so.5 -> libply-splash-graphics.so.5.0.0\n",
      "\tlibcap-ng.so.0 -> libcap-ng.so.0.0.0\n",
      "\tlibpixman-1.so.0 -> libpixman-1.so.0.40.0\n",
      "\tlibtiff.so.5 -> libtiff.so.5.7.0\n",
      "\tlibdrm.so.2 -> libdrm.so.2.4.0\n",
      "\tlibgraphite2.so.3 -> libgraphite2.so.3.2.1\n",
      "\tlibctf-nobfd.so.0 -> libctf-nobfd.so.0.0.0\n",
      "\tlibfido2.so.1 -> libfido2.so.1.10.0\n",
      "\tlibXrender.so.1 -> libXrender.so.1.3.0\n",
      "\tliblzo2.so.2 -> liblzo2.so.2.0.0\n",
      "\tlibcom_err.so.2 -> libcom_err.so.2.1\n",
      "\tlibfontenc.so.1 -> libfontenc.so.1.0.0\n",
      "\tlibpango-1.0.so.0 -> libpango-1.0.so.0.5000.6\n",
      "\tlibbsd.so.0 -> libbsd.so.0.11.5\n",
      "\tlibmenuw.so.6 -> libmenuw.so.6.3\n",
      "\tlibz.so.1 -> libz.so.1.2.11\n",
      "\tlibstdc++.so.6 -> libstdc++.so.6.0.30\n",
      "\tlibicutest.so.70 -> libicutest.so.70.1\n",
      "\tlibGLX.so.0 -> libGLX.so.0.0.0\n",
      "\tlibdebconfclient.so.0 -> libdebconfclient.so.0.0.0\n",
      "\tlibfastjson.so.4 -> libfastjson.so.4.3.0\n",
      "\tlibdbus-1.so.3 -> libdbus-1.so.3.19.13\n",
      "\tlibnfnetlink.so.0 -> libnfnetlink.so.0.2.0\n",
      "\tlibgdk-3.so.0 -> libgdk-3.so.0.2404.29\n",
      "\tlibXdmcp.so.6 -> libXdmcp.so.6.0.0\n",
      "\tlibglib-2.0.so.0 -> libglib-2.0.so.0.7200.4\n",
      "\tlibncurses.so.6 -> libncurses.so.6.3\n",
      "\tlibtinfo.so.6 -> libtinfo.so.6.3\n",
      "\tlibxxhash.so.0 -> libxxhash.so.0.8.1\n",
      "\tlibcurl-gnutls.so.4 -> libcurl-gnutls.so.4.7.0\n",
      "\tlibopcodes-2.38-system.so -> libopcodes-2.38-system.so\n",
      "\tlibXft.so.2 -> libXft.so.2.3.4\n",
      "\tlibisc-export.so.1105 -> libisc-export.so.1105.0.2\n",
      "\tlibprocps.so.8 -> libprocps.so.8.0.3\n",
      "\tlibnettle.so.8 -> libnettle.so.8.4\n",
      "\tlibcc1.so.0 -> libcc1.so.0.0.0\n",
      "\tlibgtkd-3.so.0 -> libgtkd-3.so.0.10.0\n",
      "\tlibxcb-sync.so.1 -> libxcb-sync.so.1.0.0\n",
      "\tliblua5.3-c++.so.0 -> liblua5.3-c++.so.0.0.0\n",
      "\tlibusb-1.0.so.0 -> libusb-1.0.so.0.3.0\n",
      "\tlibevent_core-2.1.so.7 -> libevent_core-2.1.so.7.0.1\n",
      "\tlibssh.so.4 -> libssh.so.4.8.7\n",
      "\tlibbfd-2.38-system.so -> libbfd-2.38-system.so\n",
      "\tlibepoxy.so.0 -> libepoxy.so.0.0.0\n",
      "\tlibpanelw.so.6 -> libpanelw.so.6.3\n",
      "\tlibphobos2-ldc-shared.so.98 -> libphobos2-ldc-shared.so.2.0.98\n",
      "\tlibassuan.so.0 -> libassuan.so.0.8.5\n",
      "\tlibXpm.so.4 -> libXpm.so.4.11.0\n",
      "\tlibdevmapper.so.1.02.1 -> libdevmapper.so.1.02.1\n",
      "\tlibgobject-2.0.so.0 -> libgobject-2.0.so.0.7200.4\n",
      "\tlibuv.so.1 -> libuv.so.1.0.0\n",
      "\tlibgstcontroller-1.0.so.0 -> libgstcontroller-1.0.so.0.2003.0\n",
      "\tlibns-9.18.18-0ubuntu0.22.04.2-Ubuntu.so -> libns-9.18.18-0ubuntu0.22.04.2-Ubuntu.so\n",
      "\tlibnsl.so.2 -> libnsl.so.2.0.1\n",
      "\tlibgstbase-1.0.so.0 -> libgstbase-1.0.so.0.2003.0\n",
      "\tlibarchive.so.13 -> libarchive.so.13.6.0\n",
      "\tlibgirepository-1.0.so.1 -> libgirepository-1.0.so.1.0.0\n",
      "\tlibvted-3.so.0 -> libvted-3.so.0.10.0\n",
      "\tlibnewt.so.0.52 -> libnewt.so.0.52.21\n",
      "\tlibdatrie.so.1 -> libdatrie.so.1.4.0\n",
      "\tlibparted.so.2 -> libparted.so.2.0.3\n",
      "\tlibunwind-ptrace.so.0 -> libunwind-ptrace.so.0.0.0\n",
      "\tlibsemanage.so.2 -> libsemanage.so.2\n",
      "\tlibdw.so.1 -> libdw-0.186.so\n",
      "\tlibgd.so.3 -> libgd.so.3.0.8\n",
      "\tlibcolord.so.2 -> libcolord.so.2.0.5\n",
      "\tlibvte-2.91.so.0 -> libvte-2.91.so.0.6800.0\n",
      "\tlibcryptsetup.so.12 -> libcryptsetup.so.12.7.0\n",
      "\tlibncursesw.so.6 -> libncursesw.so.6.3\n",
      "\tlibmd.so.0 -> libmd.so.0.0.5\n",
      "\tliblua5.2-lpeg.so.2 -> liblua5.2-lpeg.so.2.0.0\n",
      "\tlibnode.so.72 -> libv8_libsampler.so\n",
      "\tlibX11.so.6 -> libX11.so.6.4.0\n",
      "\tlibudev.so.1 -> libudev.so.1.7.2\n",
      "\tlibntfs-3g.so.89 -> libntfs-3g.so.89.0.0\n",
      "\tlibfontconfig.so.1 -> libfontconfig.so.1.12.0\n",
      "\tlibatk-bridge-2.0.so.0 -> libatk-bridge-2.0.so.0.0.0\n",
      "\tlibxcb-randr.so.0 -> libxcb-randr.so.0.1.0\n",
      "\tlibpipeline.so.1 -> libpipeline.so.1.5.5\n",
      "\tlibnl-3.so.200 -> libnl-3.so.200.26.0\n",
      "\tlibappstream.so.4 -> libappstream.so.0.15.2\n",
      "\tlibgthread-2.0.so.0 -> libgthread-2.0.so.0.7200.4\n",
      "\tlibgnutls.so.30 -> libgnutls.so.30.31.0\n",
      "\tlibgomp.so.1 -> libgomp.so.1.0.0\n",
      "\tliblsan.so.0 -> liblsan.so.0.0.0\n",
      "\tlibmenu.so.6 -> libmenu.so.6.3\n",
      "\tlibcairo.so.2 -> libcairo.so.2.11600.0\n",
      "\tlibgpg-error.so.0 -> libgpg-error.so.0.32.1\n",
      "\tlibxtables.so.12 -> libxtables.so.12.4.0\n",
      "\tliblua5.3.so.0 -> liblua5.3.so.0.0.0\n",
      "\tlibnotify.so.4 -> libnotify.so.4.0.0\n",
      "\tlibgdbm.so.6 -> libgdbm.so.6.0.0\n",
      "\tlibunistring.so.2 -> libunistring.so.2.2.0\n",
      "\tlibunwind-x86_64.so.8 -> libunwind-x86_64.so.8.0.1\n",
      "\tlibply.so.5 -> libply.so.5.0.0\n",
      "\tlibaudit.so.1 -> libaudit.so.1.0.0\n",
      "\tlibGLX_mesa.so.0 -> libGLX_mesa.so.0.0.0\n",
      "\tlibpcre2-8.so.0 -> libpcre2-8.so.0.10.4\n",
      "\tlibzstd.so.1 -> libzstd.so.1.4.8\n",
      "\tlibreadline.so.8 -> libreadline.so.8.1\n",
      "\tlibatm.so.1 -> libatm.so.1.0.0\n",
      "\tlibjson-c.so.5 -> libjson-c.so.5.1.0\n",
      "\tlibXt.so.6 -> libXt.so.6.0.0\n",
      "\tlibsepol.so.2 -> libsepol.so.2\n",
      "\tlibglapi.so.0 -> libglapi.so.0.0.0\n",
      "\tlibxcb-render.so.0 -> libxcb-render.so.0.0.0\n",
      "\tlibgssapi_krb5.so.2 -> libgssapi_krb5.so.2.2\n",
      "\tlibgstnet-1.0.so.0 -> libgstnet-1.0.so.0.2003.0\n",
      "\tlibbpf.so.0 -> libbpf.so.0.5.0\n",
      "\tlibperl.so.5.34 -> libperl.so.5.34.0\n",
      "\tlibmnl.so.0 -> libmnl.so.0.2.0\n",
      "\tlibxcb-shape.so.0 -> libxcb-shape.so.0.0.0\n",
      "/lib: (from <builtin>:0)\n",
      "/sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied\n",
      "fatal: destination path 'gemma.cpp' already exists and is not an empty directory.\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Performing Test ATOMICS_LOCK_FREE_INSTRUCTIONS\n",
      "-- Performing Test ATOMICS_LOCK_FREE_INSTRUCTIONS - Success\n",
      "-- Performing Test HWY_EMSCRIPTEN\n",
      "-- Performing Test HWY_EMSCRIPTEN - Failed\n",
      "-- Performing Test HWY_RISCV\n",
      "-- Performing Test HWY_RISCV - Failed\n",
      "-- Looking for sys/auxv.h\n",
      "-- Looking for sys/auxv.h - found\n",
      "-- Looking for asm/hwcap.h\n",
      "-- Looking for asm/hwcap.h - not found\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/highway-build/googletest-download\n",
      "[ 11%] \u001b[34m\u001b[1mCreating directories for 'googletest'\u001b[0m\n",
      "[ 22%] \u001b[34m\u001b[1mPerforming download step (git clone) for 'googletest'\u001b[0m\n",
      "Cloning into 'googletest-src'...\n",
      "HEAD is now at 43efa0a4 Merge pull request #3617 from Bagira80:fix_3616\n",
      "[ 33%] \u001b[34m\u001b[1mPerforming update step for 'googletest'\u001b[0m\n",
      "[ 44%] \u001b[34m\u001b[1mNo patch step for 'googletest'\u001b[0m\n",
      "[ 55%] \u001b[34m\u001b[1mNo configure step for 'googletest'\u001b[0m\n",
      "[ 66%] \u001b[34m\u001b[1mNo build step for 'googletest'\u001b[0m\n",
      "[ 77%] \u001b[34m\u001b[1mNo install step for 'googletest'\u001b[0m\n",
      "[ 88%] \u001b[34m\u001b[1mNo test step for 'googletest'\u001b[0m\n",
      "[100%] \u001b[34m\u001b[1mCompleted 'googletest'\u001b[0m\n",
      "[100%] Built target googletest\n",
      "-- Found Python: /bin/python3.10 (found version \"3.10.12\") found components: Interpreter \n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- VERSION: 0.2.0\n",
      "-- Not Found TCMalloc: TCMALLOC_LIB-NOTFOUND\n",
      "-- Using the multi-header code from /mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/json-src/include/\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build\n",
      "[  0%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/aligned_allocator.cc.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/nanobenchmark.cc.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/per_target.cc.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/print.cc.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/targets.cc.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy.dir/hwy/timer.cc.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32m\u001b[1mLinking CXX static library libhwy.a\u001b[0m\n",
      "[ 18%] Built target hwy\n",
      "[ 18%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
      "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
      "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvirtual google::protobuf::uint8* google::protobuf::internal::ImplicitWeakMessage::_InternalSerialize(google::protobuf::uint8*, google::protobuf::io::EpsCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:85:28\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/mnt/c/Users/avsch/OneDrive/Documents/courses/fp/gemma.cpp/build/_deps/sentencepiece-src/third_party/protobuf-lite/message_lite.cc:419:30\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K’ specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
      "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
      "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "[ 29%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_128a.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_128d.cc.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f16a.cc.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f16d.cc.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f32a.cc.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f32d.cc.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f64a.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/bpe_model.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/char_model.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_f64d.cc.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i16a.cc.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i16d.cc.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/error.cc.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/filesystem.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/model_factory.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/model_interface.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i32a.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/normalizer.cc.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i32d.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/sentencepiece_processor.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i64a.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/unigram_model.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_i64d.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/util.cc.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv128a.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/word_model.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv128d.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object _deps/sentencepiece-build/src/CMakeFiles/sentencepiece.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv64a.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece.so\u001b[0m\n",
      "[ 77%] Built target sentencepiece\n",
      "[ 81%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_kv64d.cc.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u16a.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u16d.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u32a.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u32d.cc.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u64a.cc.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort_u64d.cc.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/image/image.cc.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object _deps/highway-build/CMakeFiles/hwy_contrib.dir/hwy/contrib/sort/vqsort.cc.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX static library libhwy_contrib.a\u001b[0m\n",
      "[ 92%] Built target hwy_contrib\n",
      "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/libgemma.dir/gemma.cc.o\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/libgemma.dir/compression/blob_store.cc.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX static library libgemma.a\u001b[0m\n",
      "[ 96%] Built target libgemma\n",
      "[100%] \u001b[32mBuilding CXX object CMakeFiles/gemma.dir/run.cc.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable gemma\u001b[0m\n",
      "[100%] Built target gemma\n"
     ]
    }
   ],
   "source": [
    "!bash ./gemma_cpp.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Reading prompt ] ...............Sure, here's a breakdown of linear regression:\n",
      "\n",
      "**What is it?**\n",
      "\n",
      "Linear regression is a statistical method used to find a straight line that best fits a set of data points. It's a powerful tool for understanding relationships between variables and making predictions based on new data points.\n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "1. **Data preparation:** You start by collecting data points, which can be represented in a table or a graph.\n",
      "2. **Forming the model:** You then create a mathematical equation that expresses the relationship between the dependent and independent variables.\n",
      "3. **Fitting the model:** Using a statistical software package, you find the values of the coefficients that minimize the sum of the squared errors between the predicted values and the actual values.\n",
      "4. **Interpretation:** The coefficients tell you how much each independent variable affects the dependent variable.\n",
      "5. **Prediction:** Once you have the model, you can use it to predict the dependent variable for new data points by plugging in the values of the independent variables.\n",
      "\n",
      "**Types of linear regression:**\n",
      "\n",
      "* **Simple linear regression:** This is when there is only one independent variable.\n",
      "* **Multiple linear regression:** This is when there are multiple independent variables.\n",
      "* **Logistic regression:** This is used when the dependent variable is categorical.\n",
      "\n",
      "**Benefits of using linear regression:**\n",
      "\n",
      "* Provides a simple and powerful way to model relationships between variables.\n",
      "* Can be used to make accurate predictions for new data points.\n",
      "* Helps to identify the most important independent variables influencing the dependent variable.\n",
      "\n",
      "**Limitations of linear regression:**\n",
      "\n",
      "* Assumes a linear relationship between the independent and dependent variables.\n",
      "* May not be suitable for complex relationships between variables.\n",
      "* Can be sensitive to outliers in the data.\n",
      "\n",
      "**Applications of linear regression:**\n",
      "\n",
      "* Predicting housing prices based on location and size\n",
      "* Identifying the relationship between exercise and weight loss\n",
      "* Analyzing customer behavior in marketing campaigns\n",
      "* Predicting future sales based on historical data\n",
      "\n",
      "I hope this explanation helps! Let me know if you have any further questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!echo \"Can you explain linear regression?\" | ./gemma_cpp/build/gemma -- --tokenizer /home/hunter/courses/fp/gemcp/4/tokenizer.spm --compressed_weights /home/hunter/courses/fp/gemcp/4/2b-it-sfp.sbs --model 2b-it --verbosity 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class GemmaCPP():\n",
    "    \"\"\"Wrapper for the C++ implementation of Gemma\"\"\"\n",
    "    \n",
    "    def __init__(self, gemma_cpp, tokenizer, compressed_weights, model):\n",
    "        self.gemma_cpp = gemma_cpp\n",
    "        self.tokenizer = tokenizer\n",
    "        self.compressed_weights = compressed_weights\n",
    "        self.model = model\n",
    "        \n",
    "    def eliminate_long_dots(self, input_string):\n",
    "        \"\"\"Eliminate long sequences of dots from the input string\"\"\"\n",
    "        # Define a regular expression pattern to match sequences of 2 or more dots\n",
    "        pattern = r'\\.{2,}'\n",
    "\n",
    "        # Replace all occurrences of the pattern with a space\n",
    "        output_string = re.sub(pattern, ' ', input_string)\n",
    "\n",
    "        return output_string.strip()\n",
    "    \n",
    "    def beautify_string(self, input_string):\n",
    "        \"\"\"Clean the input string by removing non-letter characters at the beginning\n",
    "           and isolated letters at the end after multiple spaces\"\"\"\n",
    "        # Remove non-letter characters at the beginning of the string\n",
    "        output_string = re.sub(r'^[^a-zA-Z]+', '', input_string.strip())\n",
    "\n",
    "        # Remove isolated letters at the end of the output string after multiple spaces\n",
    "        output_string = re.sub(r'\\s{3,}(.+)\\Z', '', output_string.strip())\n",
    "\n",
    "        return output_string\n",
    "        \n",
    "    def generate_text(self, prompt, *args, **kwargs):\n",
    "        \"\"\"Generate text using the cpp tokenizer and model\"\"\"\n",
    "\n",
    "        # Define the shell command\n",
    "        prompt = prompt.replace('\"', '').replace(\"'\", \"\")\n",
    "        shell_command = f'echo \"{prompt}\" | {gemma_cpp} -- --tokenizer {tokenizer} --compressed_weights {compressed_weights} --model {model} --verbosity 0'\n",
    "\n",
    "        # Execute the shell command and redirect stdout to the Python script's stdout\n",
    "        process = subprocess.Popen(shell_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "        output_text = \"\"\n",
    "        reading_block = \"[ Reading prompt ]\"\n",
    "        \n",
    "        # Communicate with the process and capture stdout \n",
    "        for k, char in enumerate( iter(lambda: process.stdout.read(1), b'') ):\n",
    "            single_char = char.decode(sys.stdout.encoding)\n",
    "            output_text += single_char\n",
    "            if len(output_text) % 20 == 0:\n",
    "                count_reading_blocks = output_text.count(reading_block)\n",
    "                if count_reading_blocks > 1:\n",
    "                    break\n",
    "                    \n",
    "        # Remove long sequences of dots and the reading block, beautify the string\n",
    "        output_text = output_text.replace(reading_block, \"\")\n",
    "        output_text = self.eliminate_long_dots(output_text)\n",
    "        output_text = self.beautify_string(output_text)\n",
    "        output_text = prompt + output_text\n",
    "        \n",
    "        # Return output text\n",
    "        return [output_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = \"thenlper/gte-large\"\n",
    "gemma_cpp = \"./gemma_cpp/build/gemma\"\n",
    "tokenizer = \"/home/hunter/courses/fp/gemcp/4/tokenizer.spm\"\n",
    "compressed_weights = \"/home/hunter/courses/fp/gemcp/4/2b-it-sfp.sbs\"\n",
    "model = \"2b-it\"\n",
    "\n",
    "# Create an instance of the class AIAssistant based on Gemma C++\n",
    "gemma_ai_assistant = AIAssistant(\n",
    "    gemma_model=GemmaCPP(gemma_cpp, tokenizer, compressed_weights, model),\n",
    "    embeddings_name=embeddings_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing and mapping the knowledge base:\n",
      "Mapping 15968 pieces of information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15968/15968 [1:48:27<00:00,  2.45it/s]  \n",
      "2024-04-21 21:37:22.008663: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 15968\n",
      "2024-04-21 21:37:22.130273: W scann/utils/gmm_utils.cc:921] Could not normalize centroid due to zero norm or empty or zero-weight partition.\n",
      "2024-04-21 21:37:22.949502: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 938.053292ms.\n"
     ]
    }
   ],
   "source": [
    "# Loading the previously prepared knowledge base and embeddings\n",
    "wikipedia_data_science_kb = pd.read_csv(\"wikipedia_data_science_kb.csv\")\n",
    "knowledge_base = wikipedia_data_science_kb.wikipedia_text.tolist()\n",
    "\n",
    "# Map the intended knowledge base to embeddings and index it\n",
    "gemma_ai_assistant.learn_knowledge_base(knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to disk (for later use)\n",
    "gemma_ai_assistant.save_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 21:43:13.017657: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 15968\n",
      "2024-04-21 21:43:13.856298: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 838.534081ms.\n"
     ]
    }
   ],
   "source": [
    "### Using embedding for future use:\n",
    "gemma_ai_assistant.load_embeddings(filename=\"embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the temperature (creativity) of the AI assistant and set the role\n",
    "gemma_ai_assistant.set_temperature(0.0)\n",
    "gemma_ai_assistant.set_role(\"data science expert whose explanations are useful, clear and complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AIAssistant' object has no attribute 'searcher'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgemma_ai_assistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIn short, what are the key differences between gradient boosting and random forests?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 51\u001b[0m, in \u001b[0;36mAIAssistant.query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Query the knowledge base of the AI assistant.\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Generate and print an answer to the query\u001b[39;00m\n\u001b[1;32m     49\u001b[0m answer \u001b[38;5;241m=\u001b[39m generate_summary_and_answer(query, \n\u001b[1;32m     50\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknowledge_base, \n\u001b[0;32m---> 51\u001b[0m                                      \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearcher\u001b[49m, \n\u001b[1;32m     52\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model, \n\u001b[1;32m     53\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgemma_model,\n\u001b[1;32m     54\u001b[0m                                      temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m     55\u001b[0m                                      role\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AIAssistant' object has no attribute 'searcher'"
     ]
    }
   ],
   "source": [
    "gemma_ai_assistant.query(\"In short, what are the key differences between gradient boosting and random forests?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
